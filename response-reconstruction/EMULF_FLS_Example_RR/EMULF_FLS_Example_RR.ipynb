{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Jupyter notebook example demonstrates how to run a time domain response reconstruction analysis in Sesam, using Sesam Core. This example is based on the EMULF Delta Floater model.\n",
    "\n",
    "OneWorkflow is used to orchestrate the analysis. \n",
    "\n",
    "The analysis in this notebook contains the following steps:\n",
    "1. Run pre-compute in Sesam Core. This is done once.\n",
    "2. Read design load case information from an Excel spreadsheet\n",
    "3. For each design load case, run Response Reconstruction and fatigue analysis in Sesam Core\n",
    "4. FLS result accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_misc/EMULF_Deltafloater.png\" alt=\"image\" width=\"30%\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Input variables\n",
    "\n",
    "Some variables are defined up front. By default, they are set to values corresponding to the example model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "root_folder = Path(os.getcwd())\n",
    "\n",
    "### USER INPUT ###\n",
    "\n",
    "# Use these variables to control which parts of the pipeline you want to run\n",
    "RunPrecompute = True\n",
    "RunReconstruction = True\n",
    "RunAccumulation = True\n",
    "\n",
    "# ID of this OneWorkflow workspace\n",
    "workspace_id = \"ResponseReconstructionFLS\"  # workspace id\n",
    "\n",
    "# True if you want to run on cloud, False if you want to run locally (Note: Cloud support is coming soon!)\n",
    "cloud_run = False\n",
    "\n",
    "# Path to the workspace folder, containing all the analysis input and output files\n",
    "workspace_path = root_folder / \"Workspace\"\n",
    "\n",
    "# Prefix to the FEM file\n",
    "fem_file_prefix = \"\"\n",
    "\n",
    "# Super element number of the top super element\n",
    "top_sup = 3\n",
    "\n",
    "### Precompute part\n",
    "# Folder where the precompute will be run\n",
    "precom_execution_folder = r\"Precompute\\Execution\"\n",
    "\n",
    "# Folder where the precompute output will be stored\n",
    "precom_output_folder = r\"Precompute\\Output\"\n",
    "\n",
    "# Path to the CommonFiles folder. In this example, all input files are found here\n",
    "common_files_folder = \"CommonFiles\"\n",
    "\n",
    "# Name of Excel file containing DLC information\n",
    "excel_file = \"parameter_input.xlsx\"\n",
    "\n",
    "### END USER INPUT ###\n",
    "\n",
    "\n",
    "# Add the PythonModules folder to the system path so that we can import modules from there\n",
    "sys.path.append(str(Path(root_folder, \"../PythonModules\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-computation in Sesam Core is run once\n",
    "\n",
    "Declare required folders and variables for pre-computation, generate an input file, and execute Sesam Core pre-computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from dnv.sesam.commands import SesamCoreCommand\n",
    "from dnv.sesam.commands.executors import execute_command\n",
    "from dnv.oneworkflow import CreateInputFileFromFileTemplateCommand, CommandInfo\n",
    "from dnv.oneworkflow.utils import (\n",
    "    one_workflow_client,\n",
    "    run_managed_commands_in_parallel_async,\n",
    "    run_managed_commands_in_sequence_async,\n",
    ")\n",
    "\n",
    "# Define the paths to the folders and files\n",
    "execution_folder_path = workspace_path / precom_execution_folder\n",
    "output_folder_path = workspace_path / precom_output_folder\n",
    "input_folder_path = workspace_path / common_files_folder\n",
    "precompute_input_file = execution_folder_path / \"precompute_input.json\"\n",
    "precompute_input_file_template = \"precompute_input_template.json\"\n",
    "\n",
    "# Deleting existing analysis folder, unless user doesn't want to run pre-compute\n",
    "if RunPrecompute is True:\n",
    "    shutil.rmtree(execution_folder_path, ignore_errors=True)\n",
    "    shutil.rmtree(output_folder_path, ignore_errors=True)\n",
    "\n",
    "# Ensure that the folders exist\n",
    "execution_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "output_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Note the use of Path.as_posix(): This will write the paths with forward slashes ('/') into the input file\n",
    "# The alternative is to make sure that the paths contain double back-slashes ('\\\\') in the json file\n",
    "precompute_parameters = {\n",
    "    \"inputpath\": str(input_folder_path.as_posix()),\n",
    "    \"outputpath\": str(output_folder_path.as_posix()),\n",
    "    \"executepath\": str(execution_folder_path.as_posix()),\n",
    "    \"simaforce\": str((input_folder_path / \"sima.force\").as_posix()),\n",
    "    \"femfile\": str((input_folder_path / f\"{fem_file_prefix}T{top_sup}.FEM\").as_posix()),\n",
    "}\n",
    "\n",
    "if RunPrecompute:\n",
    "    # Create a command to generate the precompute_input.json by taking the precompute_input_template.json and replacing the strings with parameters\n",
    "    precompute_input_command = CreateInputFileFromFileTemplateCommand(\n",
    "        template_input_file = precompute_input_file_template,\n",
    "        input_filename = str(precompute_input_file),\n",
    "        parameters = precompute_parameters,\n",
    "        working_dir = str(input_folder_path),\n",
    "    )\n",
    "\n",
    "    print(\"Generating input files for precompute\")\n",
    "    execute_command(precompute_input_command)\n",
    "    print(\"Input files for precompute generated\")\n",
    "\n",
    "    score_accumulate = SesamCoreCommand(\n",
    "        working_dir = str(execution_folder_path)\n",
    "    )\n",
    "\n",
    "    score_accumulate.arguments = (\n",
    "        \"precompute unit-response-json -i \" + str(precompute_input_file)\n",
    "    )\n",
    "    print(\"Running precompute command\")\n",
    "    execute_command(score_accumulate)\n",
    "    print(\"Precompute command finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying that the pre-computation was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysisStatusChecker\n",
    "\n",
    "print(\"Verifying pre-compute command result:\")\n",
    "logFileToCheck = output_folder_path / \"SCORE.MLG\"\n",
    "score_succeeded_text = \"Duration:\"\n",
    "if analysisStatusChecker.checkIfAnalysisOk(score_succeeded_text, str(logFileToCheck)):\n",
    "    print(\"Precompute run succeeded\")\n",
    "else:\n",
    "    raise Warning|(\n",
    "        \"Precompute run failed - please check the log file for more information: \"\n",
    "        + str(logFileToCheck)\n",
    "    )\n",
    "\n",
    "precompute_output_files = output_folder_path.glob(\"*.sin\")\n",
    "if not precompute_output_files:\n",
    "    raise FileNotFoundError(\"No precompute .sin files were found.\")\n",
    "\n",
    "print(\"The following .sin files were found:\")\n",
    "sin_count = 0\n",
    "for file in precompute_output_files:\n",
    "    print(file)\n",
    "    sin_count += 1\n",
    "\n",
    "# Rudimentary sanity check - there should be 8 sin files\n",
    "if sin_count < 8:\n",
    "    raise Warning(\n",
    "        \"Not all .sin files were generated! Please check the log file for more information: \"\n",
    "        + logFileToCheck\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Response Reconstruction\n",
    "1. Read parameters from Excel spreadsheet\n",
    "2. Generate necessary input files\n",
    "3. Run response reconstruction in Sesam Core for each DLC\n",
    "\n",
    "The commands for input file generation and running Sesam Core are chained together into a workflow that is executed with OneWorkflow. OneWorkflow will take care of copying the necessary input files into the execution folders, and orchestrate the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parameter_input_file = workspace_path / excel_file\n",
    "parameters_from_excel = pd.read_excel(parameter_input_file, index_col=0)\n",
    "\n",
    "# Create a OneWorkflow client\n",
    "workflow_client = one_workflow_client(\n",
    "    workspace_id = workspace_id,\n",
    "    workspace_path = str(workspace_path),\n",
    "    cloud_run = cloud_run,\n",
    "    workspace_common_folder_name = str(common_files_folder),\n",
    "    inplace_execution = True\n",
    ")\n",
    "\n",
    "local_result_path = workspace_path / workflow_client.results_directory\n",
    "lc_folder = workspace_path / workflow_client.load_cases_directory\n",
    "\n",
    "parameter_mapping = {\n",
    "    \"StartTime\": \"start_solve\",\n",
    "    \"EndTime\": \"stop_solve\",\n",
    "    \"TimeStep\": \"timestep\",\n",
    "    \"Nsteps\": \"nsteps\",\n",
    "}\n",
    "run_name = \"screeningrun1\"\n",
    "expected_output_files = []\n",
    "workflow_sequence = []\n",
    "\n",
    "for lc, case in parameters_from_excel.iterrows():\n",
    "    print(\"Processing load case: \" + lc)\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    this_lc_path = lc_folder / lc\n",
    "\n",
    "    if this_lc_path.exists() and RunReconstruction is True:\n",
    "        print(\"Load case folder already exists, deleting it\")\n",
    "        shutil.rmtree(this_lc_path, ignore_errors=True)\n",
    "        this_lc_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    expected_output_files.append(Path(this_lc_path, f\"{run_name}F{top_sup}.SIN\"))\n",
    "    expected_output_files.append(Path(this_lc_path, f\"SesamCore_{run_name}.lis\"))\n",
    "    casedict = case.to_dict()\n",
    "\n",
    "    if RunReconstruction:\n",
    "        input_parameters = {}\n",
    "\n",
    "        # find the values from the Excel sheet for give load case\n",
    "        for key, value in case.items():\n",
    "            input_parameters[parameter_mapping[key]] = str(value)\n",
    "\n",
    "        input_parameters[\"FATIGUESTART\"] = float(input_parameters[\"start_solve\"])\n",
    "        input_parameters[\"FATIGUEEND\"] = float(input_parameters[\"stop_solve\"])\n",
    "        input_parameters[\"fem\"] = f\"{fem_file_prefix}T{top_sup}.FEM\"\n",
    "        input_parameters[\"runname\"] = run_name\n",
    "\n",
    "        tasks.append(\n",
    "            CreateInputFileFromFileTemplateCommand(\n",
    "                template_input_file=\"SesamCore_screening_template.jnl\",\n",
    "                input_filename=\"SesamCore_screening.jnl\",\n",
    "                parameters=input_parameters,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        tasks.append(\n",
    "            CreateInputFileFromFileTemplateCommand(\n",
    "                template_input_file=\"input_template.json\",\n",
    "                input_filename=\"input.json\",\n",
    "                parameters=input_parameters,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        tasks.append(\n",
    "            SesamCoreCommand(\n",
    "                working_dir=str(workspace_path),\n",
    "                command=\"fatigue\",\n",
    "                input_file_name=\"input.json\",\n",
    "                options=\"-rec -s\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add the generated tasks to the workflow sequence.\n",
    "        workflow_sequence.append(\n",
    "            CommandInfo(commands=tasks, load_case_foldername=lc)\n",
    "        )\n",
    "\n",
    "if RunReconstruction:\n",
    "    print(\"Running Response Reconstruction in parallel\")\n",
    "    await run_managed_commands_in_parallel_async(\n",
    "        client=workflow_client,\n",
    "        commands_info=workflow_sequence,\n",
    "        log_job=False,\n",
    "        files_to_exclude_from_common_files_copy=[\"*.sin\"],\n",
    "        enable_common_files_copy_to_load_cases=True,\n",
    "    )\n",
    "    print(\"Finished running Response Reconstruction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that response reconstruction run was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "try:\n",
    "    os.chdir(lc_folder)\n",
    "    if not analysisStatusChecker.checkStatus(\n",
    "        {\"Sesam Core\": \"SCORE.MLG\"},\n",
    "        \"Reaction forces summed over degrees of freedom with boundary boundary constraints\",\n",
    "    ) or not analysisStatusChecker.checkExpectedFiles(expected_output_files):\n",
    "        raise Warning(\"Analysis failed, please see any relevant log files (1)\")\n",
    "except:\n",
    "    print(\"Analysis failed, please see any relevant log files\")\n",
    "finally:\n",
    "    os.chdir(cwd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FLS result accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some user settable parameters for the FLS result accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USER INPUT ###\n",
    "\n",
    "# ElementScreening, HotSpotPlate or HotSpotMultiDirectional\n",
    "fls_check_type = \"ElementScreening\"\n",
    "\n",
    "# AbsoluteOccurrence, RelativeOccurrence or Probability\n",
    "probability_mode = \"AbsoluteOccurrence\"\n",
    "\n",
    "# Accumulation period in years (not needed for AbsoluteOccurrence mode)\n",
    "accumulation_in_years = 20\n",
    "\n",
    "# Use only selected elements for calculating occurrence factor of load cases.\n",
    "use_selected_only = False\n",
    "\n",
    "# Name of the screening capacity model\n",
    "screening_capacity_model = \"fat_screen.json\"\n",
    "\n",
    "### END USER INPUT ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First generate a template .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate template file\n",
    "template_file = workspace_path / \"template.csv\"\n",
    "if template_file.exists():\n",
    "    template_file.unlink()\n",
    "\n",
    "score_accumulate = SesamCoreCommand(working_dir = str(workspace_path))\n",
    "score_accumulate.arguments = f\"accumulation generate-load-cases-template -o {str(template_file)}\"\n",
    "\n",
    "execute_command(score_accumulate)\n",
    "\n",
    "load_cases_data = pd.read_csv(template_file)\n",
    "print(load_cases_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the template file with information read from the Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lc, case in parameters_from_excel.iterrows():\n",
    "    this_lc_path = Path(lc_folder, lc)\n",
    "    sin_files = this_lc_path.glob(\"*.SIN\")\n",
    "\n",
    "    input_parameters = {}\n",
    "\n",
    "    # find the values from the Excel sheet for given load case\n",
    "    for key, value in case.items():\n",
    "        input_parameters[parameter_mapping[key]] = str(value)\n",
    "\n",
    "    for sin_file in sin_files:\n",
    "        print(f\"Found SIN file: {sin_file.relative_to(workspace_path)}\")\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"IsSelected\": [True],  # default value\n",
    "                \"Name\": [lc],\n",
    "                \"StartTime\": [input_parameters[\"start_solve\"]],\n",
    "                \"EndTime\": [input_parameters[\"stop_solve\"]],\n",
    "                \"SimulationLength\": [float(input_parameters[\"stop_solve\"]) - float(input_parameters[\"start_solve\"])],\n",
    "                \"NumberOfOccurrences\": [1],  # default value\n",
    "                \"ResultsFilePath\": [str(sin_file)],\n",
    "            }\n",
    "        )\n",
    "        with open(template_file, \"ab\") as f:\n",
    "            df.to_csv(f, header=False, index=False, sep=\";\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the FLS result accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screening_capacity_model_path = input_folder_path / screening_capacity_model\n",
    "accumulation_dir = workspace_path / \"AccumulatedResults\"\n",
    "accumulation_input_file_path = accumulation_dir / \"accumulation-input.json\"\n",
    "\n",
    "# remove accumulation folder if it exists\n",
    "if accumulation_dir.exists():\n",
    "    shutil.rmtree(accumulation_dir, ignore_errors=True)\n",
    "    accumulation_dir.mkdir()\n",
    "\n",
    "score_args = f\"accumulation generate-input \\\n",
    "               -t {fls_check_type} \\\n",
    "               -m {probability_mode} \\\n",
    "               -cm {screening_capacity_model_path} \\\n",
    "               -lcd {template_file} \\\n",
    "               -at {accumulation_in_years} \\\n",
    "               -so:{use_selected_only} \\\n",
    "               -o {accumulation_input_file_path}\"\n",
    "\n",
    "score_generate_input = SesamCoreCommand(working_dir=str(accumulation_dir))\n",
    "score_generate_input.arguments = score_args\n",
    "\n",
    "print(\"Running accumulation generate-input command\")\n",
    "execute_command(score_generate_input)\n",
    "\n",
    "score_accumulate = SesamCoreCommand(working_dir=str(accumulation_dir))\n",
    "score_accumulate.arguments = (\n",
    "    f\"accumulation accumulate -i {accumulation_input_file_path} -co\"\n",
    ")\n",
    "\n",
    "print(\"Running accumulation command\")\n",
    "execute_command(score_accumulate)\n",
    "print(\"Finished running accumulation command\")\n",
    "print(\"Accumulated results can be found in: \" + str(accumulation_dir))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
