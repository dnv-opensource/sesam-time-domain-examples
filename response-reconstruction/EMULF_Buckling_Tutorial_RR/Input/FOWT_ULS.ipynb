{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is part of the \"Buckling Analysis of EMULF Delta Floater using the Time Domain Response Reconstruction Method\" tutorial. In the previous steps of this tutorial the user is assumed to have exported a structural and a capacity model from GeniE, as well as panel models and wadam input files from HydroD. After these, a few steps are necessary in HydroD and Sima/Bladed (Sima is assumed in this tutorial). Those steps are not part of this tutorial and can be skipped as the resulting input files are provided. \n",
    "\n",
    "In this notebook Python is used with OneWorkflow to set up a Sesam Time Domain Response Reconstruction workflow for buckling check according to DNV-RP-C201. This workflow consists of a precomputation step, which is done once, and a reconstruction step, which is done for each design load case (DLC).\n",
    " \n",
    "In the precomputation step, Sesam Core is used to generate unit responses of the structure associated with unit loads, unit motions, and unit waves, accounting for mooring/tower/Morison forces, and radiated and diffracted hydrodynamic pressure, respectively. Sesam Core in this case runs Wadam and Sestra in the background. \n",
    " \n",
    "In the reconstruction step, and following a coupled analysis (computing forces and motions) in Sima, the local structure response is directly reconstructed using the precomputed unit reponses in combination with the coupled analysis results.\n",
    " \n",
    "As there is no need to generate new loads or perform structural analysis for each new design load case, the method is orders-of-magnitude faster than a standard time domain method.\n",
    "\n",
    "The capacity model comes as a single JSON file from GeniE, control data is read from a spreadsheet into a dictionary (Python), including file names for coupled analysis result.\n",
    "\n",
    "<img src=\"EMULF_Deltafloater.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "Results are currently stored in one .csv and one .lis file for each stiffened plate group.\n",
    "Buckling results are presented as a table of worst usage factor over all time steps and load cases for each stiffened plate group. Note that the first two time steps are ignored, to avoid unrealistic result peaks.\n",
    "\n",
    "The workflow is split into several cells which quickly summarize the procedure:\n",
    "\n",
    "- Set up files and folders for the workflow\n",
    "- Precompute\n",
    "- Read DLCs from a spreadsheet\n",
    "- Create tasks for each DLC\n",
    "- Run the analysis\n",
    "- Buckling results agreggation over all DLCs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up files and folders for the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some general workflow properties are setup and the OneWorkflow client is created. The client will be important to manage and execute the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "from dnv.oneworkflow.utils import *\n",
    "\n",
    "\n",
    "####### USER INPUT #########\n",
    "\n",
    "workspaceId = \"FOWT_ULS\"                    # Set the Id for the workspace\n",
    "cloudRun = False                            # Set whether to run in the cloud ( in this example set to False)\n",
    "topsuper = 3                                # superelement type number for the structural model\n",
    "model_prefix =\"\"                            # prefix for the structural model\n",
    "\n",
    "#### END of USER INPUT ####\n",
    "\n",
    "\n",
    "root_folder = os.getcwd()                                   #The root folder is set as the path to the current working directory of the notebook. \n",
    "workspacePath = str(Path(root_folder, 'Workspace'))         #The workspace path is set to a subfolder \"Workspace\" inside the root folder\n",
    "sys.path.append(os.path.join(root_folder, '..\\\\..\\\\PythonModules'))  # Path to PythonModules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related to the folders defined above, it is assumed in the code that common input files for the workflow are gathered in a folder \"CommonFiles\" inside the workspace path. If you have not yet, please copy the folder \"CommonFiles\" from the input files of this tutorial inside the \"Workspace\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precompute\n",
    "Sesam Core runs Wadam and Sestra in the background to produce unit-motion response (FD) for\n",
    "* diffraction (all selected headings) ​(DiR#.SIN)\n",
    "* radiation and inertia (6 rigid-body dofs) for user-selected frequencies (RaXX#.SIN) \n",
    "* hydrostatic pressure and gravity response ​(DiR#.SIN)\n",
    "* mooring loads and tower forces (UnitNodalR#.SIN) for nodes specified in sima.force\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess,os, shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "from dnv.sesam.commands import *\n",
    "from dnv.sesam.commands.executors import execute_command\n",
    "from dnv.oneworkflow import *\n",
    "\n",
    "####### USER INPUT #########\n",
    "\n",
    "precom_execution_folder = r\"Precompute\\\\Execution\"    # Folder where Precompute will be run\n",
    "precom_output_folder =  r\"Precompute\\\\Output\"         # Folder where output of precompute is to be stored\n",
    "precom_input_folder = r\"Input\\\\precompute\"            # Folder where input for precompute is stored\n",
    "precompute_command = \"precompute unit-response-json\"  # Command line arguments for Sesam Core to run precompute of unit response \n",
    "\n",
    "#### END of USER INPUT ####\n",
    "\n",
    "\n",
    "## Create execution and ouptut folders\n",
    "execution_folder_path = os.path.join(workspacePath, precom_execution_folder)\n",
    "output_folder_path = os.path.join(workspacePath, precom_output_folder )\n",
    "input_folder_path = os.path.join(workspacePath, precom_input_folder)\n",
    "shutil.rmtree(execution_folder_path, ignore_errors=True)\n",
    "shutil.rmtree(output_folder_path, ignore_errors=True)\n",
    "os.makedirs(execution_folder_path, exist_ok=True)\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "precompute_input_file = os.path.join(execution_folder_path, \"precompute_input.json\")\n",
    "\n",
    "\n",
    "## Create a python dictionary then replace the precompute template file with the strings from the dictionary\n",
    "## All paths are relative to the workspace folder\n",
    "precompute_parameters= {'inputpath': precom_input_folder,                   \n",
    "                        'outputpath': precom_output_folder,\n",
    "                        'executepath': precom_execution_folder,\n",
    "                        'simaforce': precom_input_folder + r\"\\\\sima.force\",\n",
    "                        'femfile' : precom_input_folder + r\"\\\\\" + model_prefix + \"T\" + str(topsuper) + \".FEM\"}\n",
    "\n",
    "precompute_input_command = CreateInputFileFromFileTemplateCommand(               # Create a command to generate the precompute_input.json by taking the precompute_input_template.json and replacing the strings with parameters for the DLC\n",
    "        template_input_file  = \"precompute_input_template.json\",\n",
    "        input_filename  = precompute_input_file,\n",
    "        parameters= precompute_parameters,\n",
    "        working_dir = input_folder_path\n",
    "    )\n",
    "execute_command(precompute_input_command)                                        # Execute the command     \n",
    "\n",
    "## Run precompute unit response in Sesam Core\n",
    "print(\"Running Sesam Core in folder \" + execution_folder_path)\n",
    "score_command = SesamCoreCommand(working_dir=workspacePath)                      # create a new Sesam Core command\n",
    "score_command.arguments = precompute_command + ' -i ' + precompute_input_file    # add command line arguments to the command (run precompute)\n",
    "execute_command(score_command)                                                   # execute the command to run precompute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if precomputation was successful\n",
    "This is a rudimentary check which succeeds if it finds the expected output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking if Sesam Core precompute command has been executed successfully')\n",
    "error = False\n",
    "\n",
    "# There should be 8 .sin files + SCORE.MLG in the output folder\n",
    "if (len(glob.glob(os.path.join(output_folder_path, \"*.sin\"))) != 8):\n",
    "    print(\"Precompute command failed - not all SIN files have been created\")\n",
    "    error = True\n",
    "if (not os.path.exists(os.path.join(output_folder_path, \"SCORE.MLG\"))):\n",
    "    print(\"Precompute command failed - cannot find SCORE.MLG file in the output folder\")\n",
    "    error = True\n",
    "\n",
    "if not error:\n",
    "    print(\"Precompute command executed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DLCs from a spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below the name of the spreadsheet is declared as well as the capacity model file.\n",
    "In the parameter mapping an association of parameters is done which will be used for assigning each entry in the spreadsheet a corresponding variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "####### USER INPUT #########\n",
    "\n",
    "dlc_sheet = \"DLC_input.xlsx\"         # The name of the spreadsheet with the parameters of each DLC\n",
    "capacity = \"EMULF_ULS_All.json\"      # Name of the capacity model\n",
    "\n",
    "#### END of USER INPUT ####\n",
    "\n",
    "## In the definition below the parameter mapping is defined which associates each column in the excel sheet with a corresponding variable in the code. \n",
    "## If you use an excel sheet with a different structure, this is where the workflow can be adjusted to interpret that sheet:\n",
    "parameter_mapping = {'Group': \"group\",\n",
    "\t\t\t\t'LoadCase': \"loadcase\",\n",
    "\t\t\t\t\"SimaFilesFolder\": \"sima_files_folder\",\n",
    "                \"TimeStep\": \"timestep\",\n",
    "\t\t\t\t\"Depth\": \"depth\",\n",
    "\t\t\t\t\"StartBuckling\": \"BUCKLINGSTART\",\n",
    "\t\t\t\t\"StopBuckling\": \"BUCKLINGEND\",\n",
    "\t\t\t\t                }\n",
    "\n",
    "input_data = pd.read_excel(os.path.join(workspacePath, dlc_sheet), index_col=1) # in here the excel sheet is read\n",
    "input_data\n",
    "\n",
    "fixed_parameters_all_load_cases = {\n",
    "    'mor_sel': topsuper,                                  # superelement number for the morison model, in this case the same as the structural model\n",
    "    'prefix': model_prefix,                               # prefix for the morison (and structural) model\n",
    "    'cap_mod': capacity,                                  # name of the capacity model, for the input.json\n",
    "    'modelf' : model_prefix+\"T\"+str(topsuper) + \".FEM\",   # name of the model file, for the input.json\n",
    "    'loadf' :  model_prefix+\"L\"+str(topsuper) + \".FEM\"    # name of the load file, for the input.json \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tasks for each DLC\n",
    "\n",
    "In the code below the tasks are created for each analysis and for each DLC. The user may select to skip part of the workflow in the USER INPUT section, in that case make sure all necessary files for the following analysis are available. Moreover, to avoid deleting these files at the beginning of the run then declare `clean = False`.\n",
    "\n",
    "- The template files are populated with the proper values for each DLC using the [`CreateInputFileFromFileTemplateCommand`](https://myworkspace.dnv.com/download/public/sesam/workflow/docs/latest/source/dnv.oneworkflow.html#dnv.oneworkflow.create_input_file_from_file_template_command.CreateInputFileFromFileTemplateCommand) (the same method was used previously to generate the input fiel for the precompute step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from dnv.sesam.commands import *\n",
    "from dnv.oneworkflow import *\n",
    "\n",
    "####### USER INPUT #########\n",
    "\n",
    "# Set True for the workflow steps to be run, and False for the steps to be skipped:\n",
    "\n",
    "run_sestra_template_command = True        # Define whether to update sestra input files with the latest variables\n",
    "run_sesam_core_template_command = True    # Define whether to update sesam core input files with the latest variables\n",
    "run_sesam_core = True                     # Define whether to run sestra and sesam core\n",
    "run_sesam_core_input_command = True\n",
    "\n",
    "clean = True                              # Define whether to clean (True) or keep (False) files (input and ouptut) from a previous run, if skipping any anlysis step this should be set to false.\n",
    "\n",
    "\n",
    "#### END of USER INPUT ####\n",
    "\n",
    "lc_path = os.path.join(workspacePath,\"LoadCases\")\n",
    "\n",
    "## Create loadcases folders\n",
    "if clean:\n",
    "    shutil.rmtree(lc_path, ignore_errors=True)\n",
    "\n",
    "workflow_sequence = []\n",
    "display(input_data)\n",
    "\n",
    "## Loop over all DLCs is run to get the parameters for each DLC, creating the DLC folders and setup the tasks.\n",
    "for casename, case in input_data.iterrows():        \n",
    "    tasks = []\n",
    "    casedict = case.to_dict()\n",
    "    input_parameters = {}\n",
    "    \n",
    "    for key, value in case.items():\n",
    "        input_parameters[parameter_mapping[key]] = str(value)\n",
    "\n",
    "    get_folder=os.path.join(workspacePath, input_parameters[\"sima_files_folder\"])     # Define the path to the folder where the Sima files for this DLC are stored\n",
    "    DLC_folder=os.path.join(lc_path, case.name)                                        # Define the path of the DLC folder, where the analysis will be run\n",
    "   \n",
    "    if not os.path.exists(DLC_folder):          \n",
    "        os.makedirs(DLC_folder)                                                       # Create the DLC folder, if it does not exist already\n",
    "    \n",
    "    shutil.copytree(get_folder, DLC_folder, dirs_exist_ok=True)                       # Copy files from the Sima files folder to the DLC folder\n",
    "\n",
    "    input_parameters = input_parameters | fixed_parameters_all_load_cases\n",
    "    print(\"The following parameters are used for load case: \" + casename)\n",
    "    print(json.dumps(input_parameters, indent=4, sort_keys=True))\n",
    " \n",
    "    sesam_core_template_command = CreateInputFileFromFileTemplateCommand(             # Create the SesamCore_buckling.jnl by taking the SesamCore_buckling_template.jnl and replacing the strings with parameters for the DLC\n",
    "        template_input_file  = \"SesamCore_buckling_template.jnl\",\n",
    "        input_filename  = \"SesamCore_buckling.jnl\",\n",
    "        parameters= input_parameters\n",
    "    )\n",
    "    if run_sesam_core_template_command:\n",
    "        tasks.append(sesam_core_template_command)\n",
    "    \n",
    "    sesam_core_input_command = CreateInputFileFromFileTemplateCommand(                # Create the SesamCore_buckling.jnl by taking the SesamCore_buckling_template.jnl and replacing the strings with parameters for the DLC\n",
    "        template_input_file  = \"input_template.json\",\n",
    "        input_filename  = \"input.json\",\n",
    "        parameters= input_parameters\n",
    "    )\n",
    "\n",
    "    if run_sesam_core_input_command:                                                  # Create the input.json by taking the inpu_template.json and replacing the strings with parameters for the DLC\n",
    "        tasks.append(sesam_core_input_command)\n",
    "\n",
    "    if run_sesam_core:\n",
    "        score_command = SesamCoreCommand(working_dir=workspacePath)    \n",
    "        score_command.arguments = \"uls -i input.json -rec -pt 8\"\n",
    "        tasks.append(score_command)\n",
    "        \n",
    "    workflow_sequence.append(CommandInfo(commands=tasks,load_case_foldername=casename)) # Add the generated tasks to the workflow sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the analysis\n",
    "\n",
    "In the code below we run the analysis using [`run_managed_commands_in_parallel_async`](https://myworkspace.dnv.com/download/public/sesam/workflow/docs/latest/source/dnv.oneworkflow.utils.html#dnv.oneworkflow.utils.workflow_commands_runners_parallel.run_managed_commands_in_parallel_async) with the previously created workflow client and the commands setup in the previous cell. This implies that analysis of each DLC will happen in parallel, as opposed to in sequence. We set `log_job=false` to get a more restricted log, but it can be set to `True` for more detailed logs if wanted. We set `enable_common_files_copy_to_load_cases=True` to copy files from the \"CommonFiles\" folder into every single DLC folder. \n",
    "\n",
    "Subsequently, the `analysisStatusChecker` is used to check whether the analysis was successful by checking Sesam Core output files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the analysis\n",
    "workflow_client = one_workflow_client(workspace_id = workspaceId, cloud_run = cloudRun, workspace_path = workspacePath, inplace_execution = True)\n",
    "print(\"Running commands in parallel\")\n",
    "await run_managed_commands_in_parallel_async(\n",
    "             client=workflow_client,\n",
    "             commands_info=workflow_sequence,\n",
    "             log_job=False,\n",
    "             enable_common_files_copy_to_load_cases=True,\n",
    " )\n",
    "\n",
    "## Check if analysis is successful \n",
    "\n",
    "local_result_path = Path(workspacePath, workflow_client.results_directory)\n",
    "prev_dir = os.getcwd()\n",
    "\n",
    "try:\n",
    "    os.chdir(local_result_path)\n",
    "    import analysisStatusChecker\n",
    "    analysisStatusChecker.checkSesamCoreStatus()\n",
    "except:\n",
    "    print('Error during analysis status check, Sesam Core may not have run successfully. Check the log files.')\n",
    "    print('Please see SCORE.MLG for more information.')\n",
    "finally:\n",
    "    os.chdir(prev_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buckling results aggregation over all DLCs\n",
    "Print maximum usage factor and criterion for worst stiffener on each plate group, over all load cases. \n",
    "\n",
    "Please note that unrealistic code check results might happen as this tutorial is work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "print(\"Current Time =\", current_time)\n",
    "print(\"Current Date =\", current_date)\n",
    "\n",
    "wd = os.getcwd()\n",
    "\n",
    "try:\n",
    "    panels_and_cases = pd.DataFrame()\n",
    "    panel_names = set()\n",
    "    for loadcase_folder_name, _ in input_data.iterrows():\n",
    "        result_folder = os.path.join(workspacePath, workflow_client.results_directory, loadcase_folder_name)\n",
    "\n",
    "        os.chdir(result_folder)\n",
    "\n",
    "        # loop over all panel result files (csv)\n",
    "        # Set threshold value for removing the first few result cases to omit extreme initial condition results\n",
    "\n",
    "        for file in glob.glob('SesamCore_buckling_*Panel*.csv'):\n",
    "            gen = pd.read_csv(file, dtype={\"math score\":int}, chunksize=10000000)\n",
    "            panel = pd.concat((x.query(\"`Result case id` >= 3\") for x in gen), ignore_index=True)\n",
    "            panel['Plate field'] = Path(file).stem\n",
    "            panel['LoadCase'] = loadcase_folder_name\n",
    "            if not panel.empty:\n",
    "                panels_and_cases = pd.concat([panels_and_cases,panel])\n",
    "                panel_names.add(Path(file).stem)\n",
    "\n",
    "except:\n",
    "    print(\"Error accumulating results, please inspect analysis resultes manually.\") \n",
    "finally:\n",
    "    os.chdir(wd)\n",
    "\n",
    "## Aggregating all results in a single table\n",
    "results_table = pd.DataFrame(columns=['Stiffener name', 'Plate field', 'UfMax', 'UfMax criterion', 'LoadCase', 'Time'])\n",
    "for panel_name in panel_names:\n",
    "    panel_results = panels_and_cases.query('`Plate field` == @panel_name')\n",
    "    panel_results.reset_index(inplace=True)\n",
    "    if not panel_results.empty:\n",
    "        results_table.loc[len(results_table.index)] = dict(panel_results.iloc[panel_results['UfMax'].idxmax()])\n",
    "        \n",
    "## Sorting by UfMax\n",
    "results_table.sort_values(by='UfMax', ascending=False, inplace=True)\n",
    "\n",
    "print(f\"Maximum Uf for each panel, taken over all load cases and result cases.\")\n",
    "print(f\"Note that the first two time steps (usually extreme results) have been filtered out to avoid reporting unrealistic peak results.\")\n",
    "\n",
    "# Adjust the display settings to show all rows\n",
    "pd.options.display.max_rows = 800\n",
    "\n",
    "## Pandas provides many different ways to visualize the data. Here we just dump the results to the output\n",
    "display(results_table)\n",
    "\n",
    "## Pandas DataFrames can be exported to a number of different output formats, such as Excel, CSV, json, XML, HTML, Parquet, etc.\n",
    "## Uncomment the lines below to experiment with different output formats\n",
    "# results_table.to_excel(os.path.join(workspacePath, 'results.xlsx'), index=False)\n",
    "# results_table.to_html(os.path.join(workspacePath, 'results.html'), index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
