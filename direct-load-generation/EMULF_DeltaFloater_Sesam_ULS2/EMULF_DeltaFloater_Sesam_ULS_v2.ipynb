{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook example demonstrates how to run buckling analysis on a model of the EMULF Delta Floater, exposed to time-dependent wave loads coming from Sima coupled analysis results via direct load generation in Wasim, using OneWorkflow locally.\n",
    "\n",
    "Note that this is an updated version (v2) of the EMULF DeltaFloater Sesam ULS example. This version runs the calm sea condition only once and is therefore more efficient. The stiffeners have eccentricity.\n",
    "\n",
    "Highlighted features of this example:\n",
    "- Read control data from Excel spreadsheet into a dictionary (Python), including file names for coupled analysis result, as well as start and stop times for Wasim direct load generation and load transfer. More control data can easily be added to the spreadsheet and transferred to relevant input files using [`CreateInputFileFromFileTemplateCommand` (OneWorkflow)](https://myworkspace.dnv.com/download/public/sesam/workflow/docs/latest/source/dnv.oneworkflow.html#dnv.oneworkflow.create_input_file_from_file_template_command.CreateInputFileFromFileTemplateCommand). Some template control data are hardcoded by user in the notebook.\n",
    "- Wasim non-linear is run, followed by a Sestra quasi-static analysis.\n",
    "- Apply safety load factors. Load factors are first read from the spreadsheet, then modified and applied to the loads resulting from two Wasim runs, where one (total) loadcase includes all load effects and one (calm sea) load case includes only permanent loads. A single Sestra analysis is then run using the scaled and combined loads. The modified factors are applied to separate L#-files and then combined in sestra.inp using the MULL card. Note that the calm sea run is run locally, whilst all other load cases (DLCs) are run using a worker (OneWorkflow).\n",
    "- One row set up for short time period direct load generation and/or short time period load transfer only. Repeat rows for multiple events and/or seeds.\n",
    "- (A threshold value limiting output of usage factor results can be applied directly on the command line in the Sesam Core task. See options in [`SesamCoreCommand`](https://myworkspace.dnv.com/download/public/sesam/workflow/docs/latest/source/dnv.sesam.commands.html#dnv.sesam.commands.sesam_core_command.SesamCoreCommand).)\n",
    "- Control Sesam Core run using commands in journal (JNL) file.\n",
    "- Graphs of reaction force in z-direction from Sestra are presented for each DLC. PNG files are automatically saved for each DLC.\n",
    "- Buckling results are presented as a table of worst usage factor over all time steps and DLCs for each stiffened plate field. The number of worst stiffeners/lines/graphs is controlled by the user. Minor detail: Note that the table also can skip a number of first the time steps, to avoid unrealistic result peaks (if present).\n",
    "- Buckling results are also shown for a user selected number of graphs starting from a user selected usage factor as time-history plots of usage factor of all stiffeners in each stiffened plate field. \n",
    "- Separate start and stop of buckling calculation in Sesam Core command journal file based on Excel input. Sestra is here run for all time steps with load mapping.\n",
    "\n",
    "The capacity model comes as a single JSON file from GeniE, and results are currently stored in one .csv and one .lis file for each stiffened plate field. Two different capacity models are provided where one is the main column with pontoon and brace and the second is the full substructure model (see middle picture below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EMULF Deltafloater](EMULF_pict.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneWorkflow setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specify <font color='red'>root_folder / workspaceId</font>.\n",
    "- Local workflow runtime temp folder path is the default: <font color='red'>C:\\Temp\\OneWorkflow</font>.\n",
    "- Specify Excel file with load case names, start/stop/time step info and safety load factors for the input_data variable (<font color='red'>control_input2.xlsx</font>).\n",
    "- (Use <font color='red'>cloudRun</font> variable to control running locally or in the cloud. (**note**: Let this be `False` until cloud support has been added and you have a valid cloud subscription.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "sys.path.append(\"../PythonModules\")\n",
    "root_folder = os.getcwd()\n",
    "\n",
    "#User defined setup, see comments above\n",
    "workspacePath = str(Path(root_folder, 'Workspace'))\n",
    "workspaceId = \"EMULF_DeltaFloater_Sesam_ULS\"\n",
    "cloudRun = False\n",
    "input_data = pd.read_excel(os.path.join(workspacePath, \"control_input2.xlsx\"), index_col=1)\n",
    "print(\"Root folder is \", root_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Excel input, update template parameters in .inp files, run Wasim and Sesam Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files and file locations:\n",
    "- Sima coupled analysis results files in the *LoadCases\\\\<load_case_name_from_excel>\\\\* folder. Results for a separate calm sea load case is required to calculate load factors.\n",
    "- All other model and input, including template files are found in the *CommonFiles* folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove existing results before running the analysis\n",
    "import shutil\n",
    "clean = True\n",
    "if clean:\n",
    "    shutil.rmtree(os.path.join(workspacePath,\"LoadCases\"), ignore_errors=True)\n",
    "shutil.copytree(os.path.join(workspacePath,\"Input\"),os.path.join(workspacePath),dirs_exist_ok=True)\n",
    "\n",
    "\n",
    "#copy files from common files to CalmSea folder\n",
    "common_files_folder = os.path.join(workspacePath,\"CommonFiles\")\n",
    "calm_sea_dir = os.path.join(workspacePath, \"LoadCases\",\"_CalmSea\")\n",
    "for filename in os.listdir(common_files_folder):\n",
    "    if filename.endswith((\"S1.FEM\", \"L1.FEM\", \".json\", \".json.log\", \".gnx\", \".jnl\")):\n",
    "        continue  # Skip files ending with what is specified\n",
    "    source_file = os.path.join(common_files_folder, filename)\n",
    "    destination_file = os.path.join(calm_sea_dir, filename)\n",
    "\n",
    "    # Check if it's a file (not a directory)\n",
    "    if os.path.isfile(source_file):\n",
    "        shutil.copy2(source_file, destination_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import os\n",
    "from dnv.sesam.commands import *\n",
    "from dnv.oneworkflow import *\n",
    "import shutil\n",
    "from WasimTaskCreator import *\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "print(current_time, current_date)\n",
    "\n",
    "# Template parameters hardcoded by user\n",
    "topsuper = 1 #top superelement number\n",
    "prefix_stru = \"STR_\" #prefix for structure files\n",
    "fixed_parameters_all_load_cases = {\n",
    "    'mor_topsel':  f\"{topsuper}\",\n",
    "    'prefix': f\"{prefix_stru}        \", #prefix, left-justified\n",
    "    'prefix_stru': f\"{prefix_stru}\", #prefix, right-justified\n",
    "    'prefix_struCalmSea': f'{prefix_stru}CalmSea',\n",
    "    'prefix_struTotal' : f'{prefix_stru}Total'\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    os.chdir(os.path.join(workspacePath,\"LoadCases\"))\n",
    "except:\n",
    "    print(\"LoadCases folder not found\")\n",
    "workflow_sequence = []\n",
    "\n",
    "\n",
    "#Recognize the following column titles in Excel spreadsheet and map them to the correct input parameters given in Wasim template files\n",
    "parameter_mapping = {'Group': \"group\",\n",
    "\t\t\t\t'LoadCase': \"loadcase\",\n",
    "\t\t\t\t\"SimaFilesFolder\": \"sima_files_folder\",\n",
    "\t\t\t\t\"SimaFilesName\": \"sima_files_name\",\n",
    "                \"TimeStep\": \"timestep\",\n",
    "\t\t\t\t\"Depth\": \"depth\",\n",
    "\t\t\t\t\"StartWasimSolve\": \"start_solve\",\n",
    "\t\t\t\t\"StopWasimSolve\": \"stop_solve\",\n",
    "            \t\"StartWasimStru\": \"start_stru\",\n",
    "\t\t\t\t\"StopWasimStru\": \"stop_stru\",\n",
    "\t\t\t\t\"NSteps\": \"nsteps\",\n",
    "\t\t\t\t\"StartSestra\": \"start_sestra\",\n",
    "\t\t\t\t\"StopSestra\": \"stop_sestra\",\n",
    "\t\t\t\t\"StartBuckling\": \"BUCKLINGSTART\",\n",
    "\t\t\t\t\"StopBuckling\": \"BUCKLINGEND\",\n",
    "\t\t\t\t\"LoadFactorSet\": \"loadfactorset\",\n",
    "                \"fac_G\": \"fac_g\",\n",
    "                \"fac_E\": \"fac_e\",\n",
    "                }\n",
    "\n",
    "#Display data read from Excel\n",
    "display(input_data)\n",
    "\n",
    "#Loop over all load cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_for_load_case(case : dict, casename : str):\n",
    "\n",
    "    input_parameters = {}\n",
    "    for key, value in case.items():\n",
    "        input_parameters[parameter_mapping[key]] = str(value)\n",
    "\n",
    "    #Calculate modified load factors since Wasim results don't separate static and dynamic loads\n",
    "    #CalmSea (named calm)  = static\n",
    "    #Total (named fac_tot) = static + dynamic\n",
    "    input_parameters[\"fac_tot\"] = str(float(input_parameters['fac_e']))\n",
    "    input_parameters[\"fac_calm\"] = str(float(input_parameters['fac_g']) - float(input_parameters['fac_e']))\n",
    "    input_parameters = input_parameters | fixed_parameters_all_load_cases\n",
    "\n",
    "    return input_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calm sea load case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "- The calm sea load case runs Wasim to create load files with the static/permanent loads.\n",
    "- The calm sea template files are slightly different to the other template files in that Wasim solve runs fixed mode and that results for only two time steps are stored.\n",
    "- The calm sea load case is always run locally (no temporary job or blob folders).\n",
    "- After Wasim has completed the S#.FEM and L#.FEM files are copied to CommonFiles. Those file are later copied to all DLC folders when running. The SCAL factor in S#.FEM is updated in each DLC after copying to reflect the relevant DLC (factors in the spreadsheet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnv.sesam.commands.executors import execute_command\n",
    "import subprocess\n",
    "from dnv.oneworkflow import CreateInputFileFromFileTemplateCommand, CommandInfo, PythonCommand\n",
    "from dnv.sesam.commands import SestraCommand\n",
    "calm_sea_tasks = []\n",
    "workflow_sequence = []\n",
    "casename = \"_CalmSea\"\n",
    "calmSeaFolder = os.path.join(workspacePath,\"LoadCases\",casename)\n",
    "for casename, case in input_data.iterrows():\n",
    "\n",
    "\n",
    "    #===============================\n",
    "    #===Use different wasim template files for calm sea run, defined on 1st line in spreadsheet and named _CalmSea.\n",
    "    if casename != \"_CalmSea\":\n",
    "         break\n",
    "\n",
    "    os.chdir(calmSeaFolder)\n",
    "    input_parameters = get_parameters_for_load_case(case.to_dict(), casename)\n",
    "    #print(\"The following parameters are used for load case: \" + casename)\n",
    "    #print(json.dumps(input_parameters, indent=4, sort_keys=False))\n",
    "    calm_sea_commands =  WasimTaskCreator().CreateTasks(input_parameters, \"CalmSea\")\n",
    "    print(\"Running Wasim including load transfer\")\n",
    "    for wasim_cmd in calm_sea_commands:\n",
    "            print(\"running \" + str(type(wasim_cmd)))\n",
    "            execute_command(wasim_cmd)\n",
    "    #Apply load factor\n",
    "    fac_calm=input_parameters[\"fac_calm\"]\n",
    "    #print(f\"running command python appendScalingFactor.py {prefix_stru}CalmSeaS1.FEM {fac_calm}\")\n",
    "    result = subprocess.run([\"python\", \"appendScalingFactor.py\", f\"{prefix_stru}CalmSeaS1.FEM\", f\"{fac_calm}\"],\n",
    "                    capture_output=True,\n",
    "                    text=True\n",
    "                    )\n",
    "    print(result.stdout)\n",
    "\n",
    "    filesToCopy = [f\"{prefix_stru}CalmSeaL1.FEM\",f\"{prefix_stru}CalmSeaS1.FEM\"]\n",
    "    commonFilesFolder = os.path.join(workspacePath,\"CommonFiles\")\n",
    "\n",
    "    for file in filesToCopy:\n",
    "        newFilePath = commonFilesFolder + os.path.sep + file\n",
    "        print(\"file copied to \" + newFilePath)\n",
    "        shutil.copy(file, newFilePath)\n",
    "    os.chdir(os.path.join(workspacePath,\"LoadCases\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start workflow worker running analysis in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnv.oneworkflow.utils import *\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "print(current_time, current_date)\n",
    "\n",
    "# local workspace, all results will be put here in the folder Loadcases, after local or cloud runs\n",
    "# location of common files for all analysis, has to be below workspacePath and in the folder names CommonFiles\n",
    "# If running locally the code below will also start the local workflow host.\n",
    "workflow_client = one_workflow_client(workspace_id = workspaceId, cloud_run = cloudRun, workspace_path = workspacePath, inplace_execution=False, max_cores = 2)\n",
    "workflow_sequence = []\n",
    "for casename, case in input_data.iterrows():\n",
    "\n",
    "\n",
    "    #===============================\n",
    "    #===Use different wasim template files for calm sea run, defined on 1st line in spreadsheet.\n",
    "    if casename != \"_CalmSea\":\n",
    "        input_parameters = get_parameters_for_load_case(case.to_dict(), casename)\n",
    "\n",
    "        #Continue with normal template files for all normal (total) runs.\n",
    "        #===============================\n",
    "\n",
    "        #Run Wasim including update of .inp files, for ULS including load factors\n",
    "        tasks = WasimTaskCreator().CreateTasks(input_parameters)\n",
    "\n",
    "        #Apply load factors\n",
    "        fac_calm=input_parameters[\"fac_calm\"]\n",
    "        fac_tot = input_parameters[\"fac_tot\"]\n",
    "        #Need to update scaling factor for calm sea according to current DLC\n",
    "        print(casename, \"fac_calm =\", fac_calm, \"fac_tot =\", fac_tot)\n",
    "        # Update CalmSeaS#-file created by Wasim_stru, edit LCOM and append SCAL card\n",
    "        scaling_command_calm = PythonCommand(\n",
    "                directory=workflow_client.common_directory,\n",
    "                filename=\"appendScalingFactor.py\",\n",
    "                args=f\"{prefix_stru}CalmSeaS1.FEM {fac_calm}\"\n",
    "                )\n",
    "        tasks.append(scaling_command_calm)\n",
    "\n",
    "        # Update TotalS#-file created by Wasim_stru, edit LCOM and append SCAL card\n",
    "        scaling_command_dyn = PythonCommand(\n",
    "                directory=workflow_client.common_directory,\n",
    "                filename=\"appendScalingFactor.py\",\n",
    "                args=f\"{prefix_stru}TotalS1.FEM {fac_tot}\"\n",
    "                )\n",
    "        tasks.append(scaling_command_dyn)\n",
    "\n",
    "        #Update Sestra .inp file\n",
    "        sestra_template_command = CreateInputFileFromFileTemplateCommand(\n",
    "            template_input_file  = \"sestra_template.inp\",\n",
    "            input_filename  = \"sestra.inp\",\n",
    "            parameters= input_parameters\n",
    "        )\n",
    "        tasks.append(sestra_template_command)\n",
    "\n",
    "        #Update Sesam Core .jnl file\n",
    "        sesam_core_template_command = CreateInputFileFromFileTemplateCommand(\n",
    "            template_input_file  = \"SesamCore_buckling_template.jnl\",\n",
    "            input_filename  = \"SesamCore_buckling.jnl\",\n",
    "            parameters= input_parameters\n",
    "        )\n",
    "        tasks.append(sesam_core_template_command)\n",
    "\n",
    "        #Run Sesam Core\n",
    "        tasks.append(SesamCoreCommand(command = \"ULS\",input_file_name= \"input.json\", options = \"-v\"))\n",
    "\n",
    "        #Run the above defined sequence of tasks\n",
    "        workflow_sequence.append(CommandInfo(commands=tasks,load_case_foldername=casename))\n",
    "\n",
    "\n",
    "print(\"Running commands in parallel\")\n",
    "await run_managed_commands_in_parallel_async(\n",
    "            client=workflow_client,\n",
    "            commands_info=workflow_sequence,\n",
    "            log_job=False,\n",
    "            files_to_exclude_from_common_files_copy= [f\"{prefix_stru}CalmSeaT1.FEM\", f\"{prefix_stru}TotalT1.FEM\"],\n",
    "            files_to_download_from_blob_to_client=FileTransferOptions(max_size=\"11124MB\",patterns=[\"**/*sestra.inp\", \"**/wasim_setup.inp\", \"**/wasim_solve.inp\",\n",
    "                                                                                                    \"**/wasim_snapshots.inp\", \"**/wasim_stru.inp\", \"**/*.csv\",\n",
    "                                                                                                    \"**/*.lis\", \"**/*.lis.bak\", \"**/*.mlg\", \"**/*.sin\", \"**/*.FEM\", \"**/*.jnl\"]),\n",
    "            enable_common_files_copy_to_load_cases=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if analysis is successful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "print(current_time, current_date)\n",
    "\n",
    "#Search different log file types for different messages\n",
    "#Report only not successful log files\n",
    "\n",
    "local__result_path = Path(workspacePath, workflow_client.results_directory)\n",
    "os.chdir(local__result_path)\n",
    "print(\"Verifying Wasim results:\")\n",
    "import analysisStatusChecker\n",
    "wasim_succeeded_text = \"FINISHED: SUCCESS\"\n",
    "wasim_files_to_check = {\"Wasim solve\":'wasim_solve.lis', \"Wasim stru\" : 'wasim_stru.lis'}\n",
    "analysisStatusChecker.checkStatus(wasim_files_to_check,wasim_succeeded_text)\n",
    "\n",
    "#Knut: How to omit checking CalmSea folder for score results?\n",
    "print(\"Verifying Sesam Core results:\")\n",
    "score_succeeded_text = \"Duration:\"\n",
    "score_to_check = {\"Sesam Core\":'SCORE.MLG'}\n",
    "analysisStatusChecker.checkStatus(score_to_check,score_succeeded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing\n",
    "Quality checks and usage factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reaction forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from datetime import datetime\n",
    "import re\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "print(current_time, current_date)\n",
    "\n",
    "def format_time(value, _):\n",
    "    return \"{:.1e}\".format(value)  # Format the time with two decimal places\n",
    "\n",
    "#Loop over all load cases and plot graph of reaction force vs time for each load case\n",
    "from itertools import islice\n",
    "# Skip the first load case using islice to slice off the first item from input_data iterrows\n",
    "for loadcase_folder_name, other in islice(input_data.iterrows(), 1, None):\n",
    "    wasimSolverStart = other['StartWasimSolve']\n",
    "    wasimSolverStop = other['StopWasimSolve']\n",
    "#for loadcase_folder_name, _ in input_data.iterrows():\n",
    "    # Reaction forces are reported in .csv file for dynamic analysis, but in sestra.lis for quasi-static analysis\n",
    "    # args=f\"{prefix_stru}_reactions_history1.csv\" #from Sestra DREA card\n",
    "    # result_path = os.path.join(workspacePath,workflow_client.results_directory, loadcase_folder_name, args)\n",
    "    # print(result_path)\n",
    "    result_folder = os.path.join(workspacePath,workflow_client.results_directory,loadcase_folder_name)\n",
    "    #print(result_folder)\n",
    "    os.chdir(result_folder)\n",
    "    data= {}\n",
    "\n",
    "    # Open and parse the file\n",
    "    section = None\n",
    "    with open(\"sestra.lis\", \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            # Identify sections based on keywords\n",
    "            if \"Reaction force sum for all result cases\" in line:\n",
    "                section = \"Reaction Force Sum\"\n",
    "                data[section] = []\n",
    "            elif \"Sum of load sum and reaction force sum\" in line:\n",
    "                section = \"Load-Reaction Sum\"\n",
    "                data[section] = []\n",
    "\n",
    "            # Process lines containing numeric values\n",
    "            elif re.match(r\".*[\\d]+.*\", line) and section:\n",
    "                data[section].append( line.split(\";\"))\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    data[\"Reaction Force Sum\"][0]\n",
    "    reaction_force_df = pd.DataFrame(data[\"Reaction Force Sum\"], columns=[\"result case\", \"tx\", \"ty\", \"tz\", \"rx\", \"ry\", \"rz\"])\n",
    "    for key, df in reaction_force_df.items():\n",
    "        reaction_force_df[key] = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    maxLoadCase = max(reaction_force_df[\"result case\"])\n",
    "    # Extract data for plotting\n",
    "    time = wasimSolverStart + reaction_force_df[\"result case\"] * (wasimSolverStop - wasimSolverStart)/maxLoadCase\n",
    "    fz = reaction_force_df[\"tz\"]\n",
    "\n",
    "    # Plot a 2D graph\n",
    "    plt.plot(time, fz, marker='.', label='FZ')\n",
    "    # Extract the \"tx\" column (which represents x values)\n",
    "    x_column = reaction_force_df[\"result case\"]\n",
    "\n",
    "    # # Add labels and title\n",
    "    plt.xlabel(f\"Time\", fontsize=10)\n",
    "    plt.ylabel('Reaction z-force', fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(format_time))\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.title(f'Load case: {loadcase_folder_name}\\n ', fontsize=14)\n",
    "    plt.legend(fontsize=10)  # Show legend with labels\n",
    "\n",
    "    # Add gridlines\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Set x-axis limits\n",
    "    plt.xlim(min(time), max(time))\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(workspacePath,workflow_client.results_directory, loadcase_folder_name,\"reaction_forces_plot.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print maximum usage factor and criterion for worst stiffener on each platefield, over all load cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "print(current_time, current_date)\n",
    "\n",
    "panels_and_cases = pd.DataFrame()\n",
    "panel_names = set()\n",
    "for loadcase_folder_name, _ in input_data.iterrows():\n",
    "    result_folder = os.path.join(workspacePath,workflow_client.results_directory, loadcase_folder_name)\n",
    "\n",
    "    os.chdir(result_folder)\n",
    "\n",
    "    # loop over all buckling result files (csv)\n",
    "\n",
    "    schema={\n",
    "        \"math score\": int\n",
    "    }\n",
    "\n",
    "    #.csv file runname is based on name given in SesamCore_buckling_template.jnl on RUN BUCKLING-CHECK line\n",
    "    for file in glob.glob('SesamCore_runname1_panel*.csv'):\n",
    "        gen = pd.read_csv(file, sep=';', dtype=schema, chunksize=10000000)\n",
    "        # Set threshold value (here >=1) for removing the first few result cases to omit initial extreme results\n",
    "        # This should be changed to\n",
    "        panel = pd.concat((x.query(\"`Result case id` >= 1\") for x in gen), ignore_index=True)\n",
    "        panel['Plate field'] = Path(file).stem\n",
    "        panel['LoadCase'] = loadcase_folder_name\n",
    "        if not panel.empty:\n",
    "            panels_and_cases = pd.concat([panels_and_cases,panel])\n",
    "            panel_names.add(Path(file).stem)\n",
    "\n",
    "#Show table of worst usage factors\n",
    "df2 = pd.DataFrame(columns=['Stiffener name','Plate field', 'UfMax','UfMax criterion','LoadCase', 'Time'])\n",
    "for panel_name in panel_names:\n",
    "    df = panels_and_cases.query('`Plate field` == @panel_name')\n",
    "    df.reset_index(inplace=True)\n",
    "    if not df.empty:\n",
    "        df2.loc[len(df2.index)] = dict(df.iloc[df['UfMax'].idxmax()])\n",
    "df2.sort_values(by='UfMax',ascending=False,inplace=True)\n",
    "#pd.set_option('display.max_rows', None)  # Set max rows to None to display all rows\n",
    "print(f\"Maximum Uf for each panel, taken over all result cases.\")\n",
    "display(df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot usage factors for each plate field (all stiffeners) over time-history of worst load case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from ipywidgets import interactive, Text, Dropdown, Layout\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "print(current_time, current_date)\n",
    "\n",
    "# Number of stiffeners/lines/graphs with worst Uf to display\n",
    "number_of_plate_fields_to_display =10\n",
    "\n",
    "# Result threshold value, e g show the first 10 graphs with maximum usage factors > than threshold value!\n",
    "threshold_for_uf = 0.2\n",
    "\n",
    "filtered_df = df2[df2['UfMax'] >= threshold_for_uf]\n",
    "highest_usage_factor =filtered_df.head(number_of_plate_fields_to_display)\n",
    "plate_fields_with_highest_uf= highest_usage_factor.head(number_of_plate_fields_to_display)[\"Plate field\"].to_list()\n",
    "#pd.set_option('display.max_rows', None)  # Set max rows to None to display all rows\n",
    "print(\"Table of worst UfMax in decreasing order. Number of lines/graphs set by user.\")\n",
    "display(highest_usage_factor)\n",
    "\n",
    "def multiplot(PlateField, LoadCase):\n",
    "    import matplotlib as mpl\n",
    "\n",
    "    # Set the default font size for all labels\n",
    "    mpl.rcParams.update({'font.size': 16})\n",
    "    stiffenernames = data_frames[PlateField]['Stiffener name'].unique()\n",
    "    category_colors = dict(zip(stiffenernames, cm.rainbow(np.linspace(0, 1, len(stiffenernames)))))\n",
    "\n",
    "    ax= data_frames[PlateField].plot(kind='scatter', x='Time', y='UfMax',  c=data_frames[PlateField]['Stiffener name'].apply(lambda x: category_colors[x]), figsize=(20, 7))\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=category)\n",
    "           for category, color in category_colors.items()]\n",
    "    ax.legend(handles=handles, title='Stiffener name')\n",
    "    ax.set_title('UfMax over time-history for worst load case. Includes all time steps.') # Also those time steps omitted in max table reporting.\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(\"UfMaxplot.png\")\n",
    "\n",
    "# for loadcase_folder_name, _ in df_cases.iterrows():\n",
    "for loadcase_folder_name in highest_usage_factor['LoadCase'].unique():\n",
    "    result_folder = os.path.join(workspacePath,workflow_client.results_directory, loadcase_folder_name)\n",
    "\n",
    "    os.chdir(result_folder)\n",
    "    data_frames = {}\n",
    "    for file in glob.glob('SesamCore_runname1_panel*.csv'):\n",
    "        try:\n",
    "            data_frames[Path(file).stem] = pd.read_csv(file, sep=';')[['Time', 'UfMax', 'Stiffener name']]\n",
    "        except:\n",
    "            print(file + \" had issues with formatting\\n\")\n",
    "\n",
    "    if not data_frames:\n",
    "        print(\"No valid cases found\")\n",
    "    else:\n",
    "        # Create a larger dropdown widget for PlateField selection\n",
    "        platefield_dropdown = widgets.Dropdown(\n",
    "        #options=list(data_frames.keys()),  # PlateField options\n",
    "        options=plate_fields_with_highest_uf,  # PlateField options\n",
    "        description='PlateField:',\n",
    "        layout=widgets.Layout(width='600px')  # Increase width\n",
    "    )\n",
    "\n",
    "    # Create an interactive plot with a larger PlateField selection box\n",
    "    interactive_plot = interactive(multiplot, PlateField=platefield_dropdown, LoadCase=loadcase_folder_name)\n",
    "    print(\"Table of worst UfMax in decreasing order. Number of lines/graphs set by user.\")\n",
    "    display(interactive_plot)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
