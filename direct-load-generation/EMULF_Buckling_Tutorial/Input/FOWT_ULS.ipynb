{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is part of the \"Time History Buckling Analysis of EMULF Delta Floater\" tutorial. The user is assumed to have exported a capacity model from GeniE, after which a few steps are necessary in HydroD and Sima/Bladed (Sima is assumed in this tutorial). Those steps are not part of this tutorial and can be skipped as the resulting input files are provided. \n",
    "\n",
    "In this notebook Python is used with OneWorkflow to set up the analysis which includes running Wasim to construct panel loads based on forces and motions previously computed in Sima. Subsequently, Sestra and SesamCore compute the structural response and verifies the capacity model against buckling according to DNV-RP-C201.\n",
    "\n",
    "The capacity model comes as a single JSON file from GeniE, control data is read from a spreadsheet into a dictionary (Python), including file names for coupled analysis result, as well as start and stop times for Wasim load reconstruction and load transfer. \n",
    "\n",
    "<img src=\"EMULF_Deltafloater.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "Results are currently stored in one .csv and one .lis file for each stiffened plate group.\n",
    "Graphics of reaction force in z-direction from Sestra are presented for each design load case (DLC).\n",
    "Buckling results are presented as a table of worst usage factor over all time steps and load cases for each stiffened plate group. Note that the first two time steps are ignored, to avoid unrealistic result peaks.\n",
    "\n",
    "The workflow is split into several cells which quickly summarize the procedure:\n",
    "\n",
    "- Set up files and folders for the workflow\n",
    "- Read DLCs from a spreadsheet\n",
    "- Create tasks for each DLC\n",
    "- Run the analysis\n",
    "- Print Z reaction force per DLC\n",
    "- Buckling results agreggation over all DLCs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up files and folders for the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some general workflow properties are setup and the OneWorkflow client is created. The client will be important to manage and execute the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "from dnv.oneworkflow.utils import *\n",
    "\n",
    "\n",
    "####### USER INPUT #########\n",
    "\n",
    "sys.path.append(\"../../PythonModules\")      # Relative path to PythonModules\n",
    "workspaceId = \"FOWT_ULS\"                    # Set the Id for the workspace\n",
    "cloudRun = False                            # Set whether to run in the cloud ( in this example set to False)\n",
    "\n",
    "#### END of USER INPUT ####\n",
    "\n",
    "\n",
    "root_folder = os.getcwd()                                   #The root folder is set as the path to the current working directory of the notebook.\n",
    "workspacePath = str(Path(root_folder, 'Workspace'))         #The workspace path is set to a subfolder \"Workspace\" inside the root folder\n",
    "\n",
    "workflow_client = one_workflow_client(workspace_id = workspaceId, cloud_run = cloudRun, workspace_path = workspacePath, inplace_execution = True,auto_deploy_option= AutoDeployOption.TEST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related to the folders defined above, it is assumed in the code that common input files for the workflow are gathered in a folder \"CommonFiles\" inside the workspace path. If you have not yet, please copy the folder \"CommonFiles\" from the input files of this tutorial inside the \"Workspace\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DLCs from a spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below the name of the spreadsheet is declared as well as the superelement type for the structural model and the prefix. It will be assumed that the structural model file is named <\"model_prefix\">T<\"topsuper\">.FEM.\n",
    "In the parameter mapping an association of parameters is done which will be used for assigning each entry in the spreadsheet a corresponding variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "####### USER INPUT #########\n",
    "\n",
    "dlc_sheet = \"DLC_input.xlsx\"    # The name of the spreadsheet with the parameters of each DLC\n",
    "topsuper = 2                    # superelement type number for the structural model\n",
    "model_prefix =\"FOWT_ULS_\"       # prefix for the structural model\n",
    "capacity = \"COL2_ULS.json\"      # Name of the capacity model\n",
    "\n",
    "#### END of USER INPUT ####\n",
    "\n",
    "# In the definition below the parameter mapping is defined which associates each column in the excel sheet with a corresponding variable in the code. If you use an excel sheet with a different structure, this is where the workflow can be adjusted to interpret that sheet:\n",
    "\n",
    "parameter_mapping = {'Group': \"group\",\n",
    "\t\t\t\t'LoadCase': \"loadcase\",\n",
    "\t\t\t\t\"SimaFilesFolder\": \"sima_files_folder\",\n",
    "\t\t\t\t\"SimaFilesName\": \"sima_files_name\",\n",
    "                \"TimeStep\": \"timestep\",\n",
    "\t\t\t\t\"Depth\": \"depth\",\n",
    "\t\t\t\t\"StartWasimSolve\": \"start_solve\",\n",
    "\t\t\t\t\"StopWasimSolve\": \"stop_solve\",\n",
    "            \t\"StartWasimStru\": \"start_stru\",\n",
    "\t\t\t\t\"StopWasimStru\": \"stop_stru\",\n",
    "\t\t\t\t\"NSteps\": \"nsteps\",\n",
    "\t\t\t\t\"StartSestra\": \"start_sestra\",\n",
    "\t\t\t\t\"StopSestra\": \"stop_sestra\",\n",
    "\t\t\t\t\"StartBuckling\": \"BUCKLINGSTART\",\n",
    "\t\t\t\t\"StopBuckling\": \"BUCKLINGEND\",\n",
    "\t\t\t\t                }\n",
    "\n",
    "input_data = pd.read_excel(os.path.join(workspacePath, dlc_sheet), index_col=1) # in here the excel sheet is read\n",
    "\n",
    "\n",
    "fixed_parameters_all_load_cases = {\n",
    "    'mor_sel': topsuper,                                             # superelement number for the morison model, in this case the same as the structural model\n",
    "    'prefix': model_prefix,                                          # prefix for the morison (and structural) model\n",
    "    'cap_mod': capacity,                                             # name of the capacity model, for the input.json\n",
    "    'modelf' : model_prefix+\"T\"+str(topsuper) + \".FEM\",          # name of the model file, for the input.json\n",
    "    'loadf' :  model_prefix+\"L\"+str(topsuper) + \".FEM\"           # name of the load file, for the input.json\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tasks for each DLC\n",
    "\n",
    "In the code below the tasks are created for each analysis and for each DLC. The user may select to skip part of the workflow in the USER INPUT section, in that case make sure all necessary files for the following analysis are available. For example, if Wasim has already been run, the user might opt to skip the Wasim run, but must ensure that the load files remain in the DLC folders. Moreover, to avoid deleting these files at the begining of the run then declare `clean = False`.\n",
    "\n",
    "- The tasks for all Wasim steps are easily created with the help of `WasimTaskCreator`. \n",
    "- The template files are populated with the proper values for each DLC using the [`CreateInputFileFromFileTemplateCommand`](https://myworkspace.dnv.com/download/public/sesam/workflow/docs/latest/source/dnv.oneworkflow.html#dnv.oneworkflow.create_input_file_from_file_template_command.CreateInputFileFromFileTemplateCommand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from WasimTaskCreator import *\n",
    "from dnv.sesam.commands import *\n",
    "from dnv.oneworkflow import *\n",
    "\n",
    "####### USER INPUT #########\n",
    "\n",
    "# Set True for the workflow steps to be run, and False for the steps to be skipped:\n",
    "\n",
    "run_wasim = True                          # Define whether to run Wasim (setup, solve, snapshots and stru)\n",
    "run_sestra_template_command = True        # Define whether to update sestra input files with the latest variables\n",
    "run_sesam_core_template_command = True    # Define whether to update sesam core input files with the latest variables\n",
    "run_sestra_sesam_core = True              # Define whether to run sestra and sesam core\n",
    "run_sesam_core_input_command = True\n",
    "\n",
    "clean = True                              # Define whether to clean (True) or keep (False) files (input and ouptut) from a previous run, if skipping any anlysis step this should be set to false.\n",
    "\n",
    "\n",
    "#### END of USER INPUT ####\n",
    "\n",
    "if clean:\n",
    "    shutil.rmtree(os.path.join(workspacePath,\"LoadCases\"), ignore_errors=True)\n",
    "\n",
    "try:                                      # Check whether the LoadCases folder exists\n",
    "    os.chdir(os.path.join(workspacePath,\"LoadCases\"))\n",
    "except:\n",
    "    print(\"LoadCases folder not found\")\n",
    "\n",
    "workflow_sequence = []\n",
    "tasks = []\n",
    "\n",
    "display(input_data)\n",
    "\n",
    "# In the below a loop over all DLCs is run to get the parameters for each DLC, creating the DLC folders and setup the tasks.\n",
    "\n",
    "for casename, case in input_data.iterrows():\n",
    "    casedict = case.to_dict()\n",
    "    input_parameters = {}\n",
    "\n",
    "    for key, value in case.items():\n",
    "        input_parameters[parameter_mapping[key]] = str(value)\n",
    "\n",
    "    get_folder=os.path.join(workspacePath,input_parameters[\"sima_files_folder\"])                               # Define the  path to the folder where the Sima files for this DLC are stored\n",
    "    DLC_folder=os.path.join(workspacePath,\"LoadCases\",case.name)                                               # Define the path of the DLC folder, where the analysis will be run\n",
    "    isExist = os.path.exists(DLC_folder)\n",
    "    if not isExist:\n",
    "        os.makedirs(DLC_folder)                                                                                # Create the DLC folder, if it does not exist already\n",
    "\n",
    "    shutil.copytree(os.path.join(get_folder),os.path.join(DLC_folder),dirs_exist_ok=True)                      # Copy files from the Sima files folder to the DLC folder\n",
    "\n",
    "    input_parameters = input_parameters | fixed_parameters_all_load_cases\n",
    "    print(\"The following parameters are used for load case: \" + casename)\n",
    "    print(json.dumps(input_parameters, indent=4, sort_keys=True))\n",
    "\n",
    "    #Wasimtaskcreator is a function that simplifies the creation of wasim tasks, it helps setup separate calm sea and dynamic runs in Wasim so that the load factors can be applied later in the analysis\n",
    "\n",
    "    if run_wasim:\n",
    "        tasks = WasimTaskCreator().CreateTasks(input_parameters)\n",
    "\n",
    "    sestra_template_command = CreateInputFileFromFileTemplateCommand(                                       # Create the sestra.inp by taking the sestra_template.inp and replacing the strings with parameters for the DLC\n",
    "        template_input_file  = \"sestra_template.inp\",\n",
    "        input_filename  = \"sestra.inp\",\n",
    "        parameters= input_parameters\n",
    "    )\n",
    "\n",
    "    if run_sestra_template_command:\n",
    "        tasks.append(sestra_template_command)\n",
    "\n",
    "    sesam_core_template_command = CreateInputFileFromFileTemplateCommand(                                   # Create the SesamCore_buckling.jnl by taking the SesamCore_buckling_template.jnl and replacing the strings with parameters for the DLC\n",
    "        template_input_file  = \"SesamCore_buckling_template.jnl\",\n",
    "        input_filename  = \"SesamCore_buckling.jnl\",\n",
    "        parameters= input_parameters\n",
    "    )\n",
    "    if run_sesam_core_template_command:\n",
    "        tasks.append(sesam_core_template_command)\n",
    "\n",
    "    sesam_core_input_command = CreateInputFileFromFileTemplateCommand(                                      # Create the SesamCore_buckling.jnl by taking the SesamCore_buckling_template.jnl and replacing the strings with parameters for the DLC\n",
    "        template_input_file  = \"input_template.json\",\n",
    "        input_filename  = \"input.json\",\n",
    "        parameters= input_parameters\n",
    "    )\n",
    "\n",
    "    if run_sesam_core_input_command:                                                                       # Create the input.json by taking the inpu_template.json and replacing the strings with parameters for the DLC\n",
    "        tasks.append(sesam_core_input_command)\n",
    "\n",
    "\n",
    "    if run_sestra_sesam_core:\n",
    "        tasks.append(SesamCoreCommand(command = \"uls\",input_file_name= \"input.json\", options = \"-v\"))\n",
    "\n",
    "    workflow_sequence.append(CommandInfo(commands=tasks,load_case_foldername=casename))                       # Add the generated tasks to the workflow sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the analysis\n",
    "\n",
    "In the code below we run the analysis using [`run_managed_commands_in_parallel_async`](https://myworkspace.dnv.com/download/public/sesam/workflow/docs/latest/source/dnv.oneworkflow.utils.html#dnv.oneworkflow.utils.workflow_commands_runners_parallel.run_managed_commands_in_parallel_async) with the previously created workflow client and the commands setup in the previous cell. We set `log_job=false` to get a more restricted log, but it can be set to `True` for more detailed logs if wanted. We set `enable_common_files_copy_to_load_cases=True` to copy files from the \"CommonFiles\" folder into every single DLC folder. \n",
    "\n",
    "Subsequently, the `analysisStatusChecker` is used to check whether the analysis was successful by reading Wasim and Sesam Core output files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the analysis\n",
    "\n",
    "print(\"Running commands in parallel\")\n",
    "await run_managed_commands_in_parallel_async(\n",
    "             client=workflow_client,\n",
    "             commands_info=workflow_sequence,\n",
    "             log_job=False,\n",
    "             enable_common_files_copy_to_load_cases=True,\n",
    " )\n",
    "\n",
    "\n",
    "## Check if analysis is successful\n",
    "\n",
    "local__result_path = Path(workspacePath, workflow_client.results_directory)\n",
    "os.chdir(local__result_path)\n",
    "\n",
    "print(\"Verifying Wasim results:\")\n",
    "wasim_succeeded_text = \"FINISHED: SUCCESS\"\n",
    "wasim_files_to_check = {\"Wasim solve\":'wasim_solve.lis', \"Wasim stru\" : 'wasim_stru.lis'}\n",
    "\n",
    "import analysisStatusChecker\n",
    "analysisStatusChecker.checkStatus(wasim_files_to_check,wasim_succeeded_text)\n",
    "\n",
    "\n",
    "print(\"Verifying Sesam Core results:\")\n",
    "score_succeeded_text = \"Duration:\"\n",
    "score_to_check = {\"Sesam Core\":'SCORE.MLG'}\n",
    "analysisStatusChecker.checkStatus(score_to_check,score_succeeded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Z reaction forces per DLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "print(\"Current Time =\", current_time)\n",
    "print(\"Current Date =\", current_date)\n",
    "\n",
    "def format_time(value, _):\n",
    "    return \"{:.1e}\".format(value)  # Format the time with two decimal places\n",
    "for loadcase_folder_name, _ in input_data.iterrows():\n",
    "    result_path = os.path.join(workspacePath,workflow_client.results_directory, loadcase_folder_name,model_prefix+\"_reactions_history1.csv\")\n",
    "    reaction_history = pd.read_csv(result_path)\n",
    "    df = pd.read_csv(result_path, delimiter=';')\n",
    "    from matplotlib import pylab as plt\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "    # Set the default font size for all labels\n",
    "\n",
    "    time = df['Time']\n",
    "    fx = df['FX']\n",
    "    fy = df['FY']\n",
    "    fz = df['FZ']\n",
    "    mx = df['MX']\n",
    "    my = df['MY']\n",
    "    mz = df['MZ']\n",
    "\n",
    "    # Define a custom formatting function for x-axis values\n",
    "\n",
    "\n",
    "    # Plot a 2D graph\n",
    "    plt.plot(time, fz, marker='.', label='FZ')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Result case id', fontsize=10)\n",
    "    plt.ylabel('Force', fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(format_time))\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.title('Reaction z-force over time-history', fontsize=14)\n",
    "    plt.legend(fontsize=10)  # Show legend with labels\n",
    "\n",
    "    # Add gridlines\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Set x-axis limits\n",
    "    plt.xlim(min(time), max(time))\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(workspacePath,workflow_client.results_directory, loadcase_folder_name,\"reaction_forces_plot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buckling results aggregation over all DLCs\n",
    "Print maximum usage factor and criterion for worst stiffener on each plate group, over all load cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "print(\"Current Time =\", current_time)\n",
    "print(\"Current Date =\", current_date)\n",
    "\n",
    "panels_and_cases = pd.DataFrame()\n",
    "panel_names = set()\n",
    "for loadcase_folder_name, _ in input_data.iterrows():\n",
    "    result_folder = os.path.join(workspacePath,workflow_client.results_directory, loadcase_folder_name)\n",
    "\n",
    "    os.chdir(result_folder)\n",
    "\n",
    "    # loop over all panel result files (csv)\n",
    "    # Set threshold value for removing the first few result cases to omit extreme initial condition results\n",
    "\n",
    "    schema={\n",
    "        \"math score\": int\n",
    "    }\n",
    "\n",
    "    for file in glob.glob('SesamCore_buckling_*Panel*.csv'):\n",
    "        gen = pd.read_csv(file, sep=';', dtype=schema, chunksize=10000000)\n",
    "        panel = pd.concat((x.query(\"`Result case id` >= 3\") for x in gen), ignore_index=True)\n",
    "        panel['Plate field'] = Path(file).stem\n",
    "        panel['LoadCase'] = loadcase_folder_name\n",
    "        if not panel.empty:\n",
    "            panels_and_cases = pd.concat([panels_and_cases,panel])\n",
    "            panel_names.add(Path(file).stem)\n",
    "\n",
    "df2 = pd.DataFrame(columns=['Stiffener name','Plate field', 'UfMax','UfMax criterion','LoadCase', 'Time'])\n",
    "for panel_name in panel_names:\n",
    "    df = panels_and_cases.query('`Plate field` == @panel_name')\n",
    "    df.reset_index(inplace=True)\n",
    "    if not df.empty:\n",
    "        df2.loc[len(df2.index)] = dict(df.iloc[df['UfMax'].idxmax()])\n",
    "df2.sort_values(by='UfMax',ascending=False,inplace=True)\n",
    "print(f\"Maximum Uf for each panel, taken over all load cases and result cases.\")\n",
    "print(f\"Note that the first two time steps (usually extreme results) have been filtered out to avoid reporting unrealistic peak results.\")\n",
    "display(df2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
